
# test.py â€” Disentanglement VC (WORLD) with Target-Domain Mean Style (matches train.py)
# ì‹¤í–‰:
#   1) MAIN CONFIGURATIONì—ì„œ input_dir / output_dir / direction ì„¤ì •
#   2) python test.py
#
# í•„ìš”:
#   - ./checkpoints_recon/content_decoder_final.pth   (train.pyê°€ ì €ì¥)
#   - ./cache/*  (ì „ì²˜ë¦¬ ìºì‹œ; ìµœì†Œ mcep_norm, logf0_norm, content/logf0_global í†µê³„)
#
# í•µì‹¬ ë³€ê²½:
#   - train.pyì™€ ë™ì¼í•˜ê²Œ SpeakerEncoderë¥¼ ì‚¬ìš©í•˜ë˜, ì¶”ë¡  ì‹œì—”
#     ìºì‹œì˜ íƒ€ê¹ƒ ë„ë©”ì¸ MCEPë“¤ì„ SpeakerEncoderì— í†µê³¼ì‹œì¼œ 'í‰ê·  ìŠ¤íƒ€ì¼ ë²¡í„°'ë¥¼ ë§Œë“¤ê³ ,
#     ê·¸ í‰ê·  ë²¡í„°ë¥¼ ContentDecoderì— ì£¼ì…í•©ë‹ˆë‹¤.
#   - SpeakerEncoder ê°€ì¤‘ì¹˜ëŠ” train.pyì™€ ë™ì¼í•œ ì´ˆê¸°í™”(ì‹œë“œ 1337)ë¡œ ìƒì„±í•©ë‹ˆë‹¤
#     (train.pyì—ì„œ spk_encoder_trainable=False ì˜€ìœ¼ë¯€ë¡œ ì €ì¥ë³¸ì´ ì—†ì–´ë„ ì¼ì¹˜í•©ë‹ˆë‹¤).

import os
import time
from dataclasses import dataclass
from typing import Optional, List, Tuple

import numpy as np
import soundfile as sf
import librosa
import torch
import torch.nn as nn
import pyworld

# ===== í”„ë¡œì íŠ¸ ëª¨ë“ˆ =====
from data_preprocess import CacheIO
from model import ContentDecoder, DecoderCfg, SpeakerEncoder, SpkEncCfg

# ===== Utils =====
def now() -> str:
    return time.strftime("%H:%M:%S")

def ensure_dir(path: str):
    os.makedirs(path, exist_ok=True)

def set_seed(seed: int = 1337):
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

def pad_wav_to_multiple(wav: np.ndarray, sr: int, frame_period_ms: float, multiple: int = 4) -> np.ndarray:
    frame_size = int(sr * frame_period_ms / 1000)
    n = len(wav)
    blocks = max(int(np.floor(n / frame_size)) + 1, 1)
    padded_frames = int((np.ceil(blocks / multiple + 1) * multiple - 1) * frame_size)
    pad = max(padded_frames - n, 0)
    if pad <= 0:
        return wav
    L = pad // 2
    R = pad - L
    return np.pad(wav, (L, R), mode="constant", constant_values=0.0)

def world_decompose(wav: np.ndarray, sr: int, frame_period_ms: float = 5.0):
    wav64 = wav.astype(np.float64)
    f0, timeaxis = pyworld.harvest(wav64, sr, frame_period_ms)
    sp = pyworld.cheaptrick(wav64, f0, timeaxis, sr)
    ap = pyworld.d4c(wav64, f0, timeaxis, sr)
    return f0, sp, ap

def world_code_sp(sp: np.ndarray, sr: int, mcep_dim: int):
    return pyworld.code_spectral_envelope(sp, sr, mcep_dim)  # (T, mcep_dim)

def world_decode_sp(coded_sp: np.ndarray, sr: int):
    fftlen = pyworld.get_cheaptrick_fft_size(sr)
    return pyworld.decode_spectral_envelope(coded_sp, sr, fftlen)

def world_synthesize(f0: np.ndarray, dec_sp: np.ndarray, ap: np.ndarray, sr: int, frame_period_ms: float = 5.0):
    y = pyworld.synthesize(f0, dec_sp, ap, sr, frame_period_ms)
    return y.astype(np.float32)

def convert_pitch_statistically(f0: np.ndarray,
                                mean_log_src: float, std_log_src: float,
                                mean_log_tgt: float, std_log_tgt: float) -> np.ndarray:
    out = np.array(f0, dtype=np.float64, copy=True)
    idx = out > 0
    if np.any(idx):
        out[idx] = np.exp((np.log(out[idx] + 1e-8) - mean_log_src) / (std_log_src + 1e-8) * std_log_tgt + mean_log_tgt)
    return out

# ===== Content Embedder (torchaudio wav2vec2-base) =====
try:
    import torchaudio
    from torchaudio.functional import resample as ta_resample
    from torchaudio.pipelines import WAV2VEC2_BASE
    _TORCHAUDIO_OK = True
except Exception:
    _TORCHAUDIO_OK = False

class ContentEmbedder:
    def __init__(self, device: torch.device):
        if not _TORCHAUDIO_OK:
            raise RuntimeError("ContentEmbedder requires torchaudio. `pip install torchaudio`")
        bundle = WAV2VEC2_BASE
        self.model = bundle.get_model().to(device).eval()
        self.target_sr = bundle.sample_rate
        self.device = device

    @torch.inference_mode()
    def extract(self, wav: np.ndarray, sr: int) -> np.ndarray:
        t = torch.from_numpy(wav).float().unsqueeze(0).to(self.device)  # (1,T)
        if sr != self.target_sr:
            t = ta_resample(t, sr, self.target_sr)
        embs = self.model(t)[0].squeeze(0).cpu().numpy()  # (T', C)
        return embs

    @staticmethod
    def align_to_length(feat_TxC: np.ndarray, L: int) -> np.ndarray:
        T, C = feat_TxC.shape
        if T == L:
            return feat_TxC.T.astype(np.float32)  # (C, L)
        xp = np.linspace(0.0, 1.0, T)
        xq = np.linspace(0.0, 1.0, L)
        out = np.empty((C, L), dtype=np.float32)
        for ch in range(C):
            out[ch] = np.interp(xq, xp, feat_TxC[:, ch])
        return out  # (C, L)

# ===== Config =====
@dataclass
class TestConfig:
    # ê²½ë¡œ
    cache_dir: str = "./cache"
    decoder_ckpt: str = "./checkpoints_recon/content_decoder_final.pth"

    # ì…ì¶œë ¥ & ë°©í–¥
    input_dir: str = "my_version_song"
    output_dir: str = "converted_outputs/my_song_50s"
    direction: str = "A2B"  # "A2B": 20/30ëŒ€ â†’ 40/50/60ëŒ€, "B2A": ì—­

    # ì˜¤ë””ì˜¤/ëª¨ë¸ íŒŒë¼ë¯¸í„° (train.pyì˜ ReconConfigì™€ ì¼ì¹˜)
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    sr: int = 16000
    frame_period: float = 5.0
    mcep_dim: int = 36
    content_dim: int = 768
    style_dim: int = 128
    channels: int = 256
    n_resblocks: int = 8
    use_vuv: bool = True

    # íƒ€ê¹ƒ ìŠ¤íƒ€ì¼ í‰ê·  ì„¤ì •
    n_style_refs: int = 16       # í‰ê· ì— ì‚¬ìš©í•  íƒ€ê¹ƒ ë„ë©”ì¸ ìƒ˜í”Œ ìˆ˜
    style_seg_len: int = 192     # ê° ìƒ˜í”Œì—ì„œ ì‚¬ìš©í•  í”„ë ˆì„ ê¸¸ì´ (trainê³¼ ë§ì¶¤)

# ===== Converter =====
class Converter:
    def __init__(self, cfg: TestConfig):
        self.cfg = cfg
        self.device = torch.device(cfg.device)

        print(f"[{now()}] ğŸ”§ ì´ˆê¸°í™” ì‹œì‘...")

        # train.pyì™€ ë™ì¼í•œ ì‹œë“œë¡œ ì´ˆê¸°í™” (SpeakerEncoder ë¬´ì €ì¥ ì‹œ ë™ì¼ íŒŒë¼ë¯¸í„° ë³´ì¥)
        set_seed(1337)

        # 1) Decoder
        self.decoder = self._load_decoder()

        # 2) SpeakerEncoder (í•™ìŠµ ë•Œì™€ ë™ì¼ êµ¬ì¡°, ê¸°ë³¸ì€ ì´ˆê¸°í™” ê³ ì •)
        self.spk_enc = SpeakerEncoder(SpkEncCfg(in_dim=36, out_dim=cfg.style_dim)).to(self.device).eval()
        spk_ckpt_guess = os.path.join(os.path.dirname(cfg.decoder_ckpt), "spk_encoder_final.pth")
        if os.path.isfile(spk_ckpt_guess):
            try:
                self.spk_enc.load_state_dict(torch.load(spk_ckpt_guess, map_location=self.device), strict=False)
                print(f"[{now()}] ğŸ™ï¸ SpeakerEncoder ë¡œë“œ: {spk_ckpt_guess}")
            except Exception as e:
                print(f"[{now()}] âš ï¸ SpeakerEncoder ë¡œë“œ ì‹¤íŒ¨(ë¬´ì‹œ): {e}  (ì´ˆê¸°í™”ëœ ê³ ì • ê°€ì¤‘ì¹˜ ì‚¬ìš©)")

        # 3) Content embedder
        self.content_embedder = ContentEmbedder(self.device)

        # 4) í†µê³„ ë¡œë“œ
        self._load_statistics()

        # 5) íƒ€ê¹ƒ ë„ë©”ì¸ í‰ê·  ìŠ¤íƒ€ì¼ ë²¡í„° ì¤€ë¹„
        self.target_style = self._build_target_mean_style().to(self.device)  # (1, style_dim)

        print(f"[{now()}] âœ… ì´ˆê¸°í™” ì™„ë£Œ.")

    def _load_decoder(self) -> ContentDecoder:
        dcfg = DecoderCfg(
            content_dim=self.cfg.content_dim, pitch_dim=1,
            vuv_dim=(1 if self.cfg.use_vuv else 0),
            style_dim=self.cfg.style_dim,
            out_dim=self.cfg.mcep_dim, channels=self.cfg.channels,
            n_resblocks=self.cfg.n_resblocks, use_film=True
        )
        decoder = ContentDecoder(dcfg).to(self.device).eval()
        path = os.path.normpath(self.cfg.decoder_ckpt)
        if not os.path.isfile(path):
            raise FileNotFoundError(f"Decoder ckpt not found: {path}")
        state = torch.load(path, map_location=self.device)
        if isinstance(state, dict) and "state_dict" in state:
            state = state["state_dict"]
        decoder.load_state_dict(state, strict=False)
        print(f"[{now()}] âœ”ï¸ Decoder ë¡œë“œ: {path}")
        return decoder

    def _load_statistics(self):
        cache = CacheIO.load_preprocessed_data(self.cfg.cache_dir)
        # í•„ìˆ˜ í†µê³„
        self.mA_mean, self.mA_std = cache["mcep_norm"]["mean_A"], cache["mcep_norm"]["std_A"]
        self.mB_mean, self.mB_std = cache["mcep_norm"]["mean_B"], cache["mcep_norm"]["std_B"]
        self.logf0_A_mean, self.logf0_A_std = float(cache["logf0_norm"]["mean_A"]), float(cache["logf0_norm"]["std_A"])
        self.logf0_B_mean, self.logf0_B_std = float(cache["logf0_norm"]["mean_B"]), float(cache["logf0_norm"]["std_B"])
        # ì„ íƒ í†µê³„ (ì—†ìœ¼ë©´ í´ë°±)
        cg = cache.get("content_normalization")
        lg = cache.get("logf0_global_normalization")
        if cg is not None:
            self.c_mean = cg["mean"].astype(np.float32)
            self.c_std  = cg["std"].astype(np.float32) + 1e-8
        else:
            self.c_mean = 0.0; self.c_std = 1.0
            print(f"[{now()}] â„¹ï¸ content_normalization ì—†ìŒ â†’ ë¬´ì •ê·œí™”")
        if lg is not None:
            self.lg_mean = float(lg["mean"]); self.lg_std = float(lg["std"]) + 1e-8
        else:
            self.lg_mean = 0.0; self.lg_std = 1.0
            print(f"[{now()}] â„¹ï¸ logf0_global_normalization ì—†ìŒ â†’ ë¬´ì •ê·œí™”")
        # íƒ€ê¹ƒ/ì†ŒìŠ¤ ë¦¬ìŠ¤íŠ¸
        self.coded_A = CacheIO.load_pickle(os.path.join(self.cfg.cache_dir, "coded_sps_A_norm.pickle"))
        self.coded_B = CacheIO.load_pickle(os.path.join(self.cfg.cache_dir, "coded_sps_B_norm.pickle"))
        if not isinstance(self.coded_A, list) or not isinstance(self.coded_B, list):
            raise RuntimeError("coded_sps_{A,B}_norm.pickle ë¡œë“œ ì‹¤íŒ¨ í˜¹ì€ í¬ë§· ì˜¤ë¥˜")

        print(f"[{now()}] âœ”ï¸ í†µê³„/ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì™„ë£Œ  (A:{len(self.coded_A)} / B:{len(self.coded_B)})")

    @torch.no_grad()
    def _build_target_mean_style(self) -> torch.Tensor:
        """
        ìºì‹œì—ì„œ íƒ€ê¹ƒ ë„ë©”ì¸ì˜ normalized MCEPë“¤ì„ ê³¨ë¼ SpeakerEncoderì— í†µê³¼,
        í‰ê·  ìŠ¤íƒ€ì¼ ë²¡í„° (1, E)ë¥¼ ë§Œë“ ë‹¤.
        """
        if self.cfg.direction.upper() == "A2B":
            pool = self.coded_B  # íƒ€ê¹ƒ: B (ì¤‘ì¥ë…„)
        elif self.cfg.direction.upper() == "B2A":
            pool = self.coded_A  # íƒ€ê¹ƒ: A (ì²­ë…„)
        else:
            raise ValueError("directionì€ 'A2B' ë˜ëŠ” 'B2A' ì—¬ì•¼ í•©ë‹ˆë‹¤.")

        if len(pool) == 0:
            raise RuntimeError("íƒ€ê¹ƒ ë„ë©”ì¸ì˜ coded_sps_*_norm ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

        n = min(self.cfg.n_style_refs, len(pool))
        segL = int(self.cfg.style_seg_len)
        idxs = np.linspace(0, len(pool)-1, n, dtype=int)  # ê· ì¼ ìƒ˜í”Œë§

        vecs = []
        for i in idxs:
            mcep_36xT = np.asarray(pool[i], dtype=np.float32)  # (36, T)
            T = mcep_36xT.shape[1]
            if T < segL:
                # pad to segL
                need = segL - T + 1
                L = max(need // 2, 0); R = max(need - L, 0)
                mcep_36xT = np.pad(mcep_36xT, ((0,0),(L,R)), mode="edge")
                T = mcep_36xT.shape[1]
            # ëœë¤/ì„¼í„° í¬ë¡­ ì¤‘ ì„¼í„° í¬ë¡­ìœ¼ë¡œ ì•ˆì •í™”
            s = (T - segL) // 2
            e = s + segL
            seg = mcep_36xT[:, s:e]  # (36, segL)
            t = torch.from_numpy(seg).unsqueeze(0).to(self.device)  # (1,36,L)
            v = self.spk_enc(t)  # (1,E)
            vecs.append(v)

        V = torch.cat(vecs, dim=0)        # (n,E)
        mean_v = V.mean(dim=0, keepdim=True)  # (1,E)
        print(f"[{now()}] ğŸ¯ íƒ€ê¹ƒ í‰ê·  ìŠ¤íƒ€ì¼ ì¤€ë¹„ ì™„ë£Œ: refs={n}, seg={segL}, norm={float(mean_v.norm().item()):.4f}")
        return mean_v  # (1,E)

    @torch.no_grad()
    def convert_file(self, wav_path: str) -> np.ndarray:
        cfg = self.cfg

        # 1) ì˜¤ë””ì˜¤ ë¡œë“œ & íŒ¨ë”©
        wav, _ = librosa.load(wav_path, sr=cfg.sr, mono=True)
        wav_pad = pad_wav_to_multiple(wav, sr=cfg.sr, frame_period_ms=cfg.frame_period)

        # 2) WORLD ë¶„í•´
        f0, sp, ap = world_decompose(wav_pad, sr=cfg.sr, frame_period_ms=cfg.frame_period)
        T = f0.shape[0]

        # 3) ì½˜í…ì¸  ì„ë² ë”©
        c_emb = self.content_embedder.extract(wav, sr=cfg.sr)              # (T', Cc)
        c_aligned = self.content_embedder.align_to_length(c_emb, T)        # (Cc, T)
        # content ì •ê·œí™”
        if isinstance(self.c_mean, np.ndarray):
            c_norm = (c_aligned - self.c_mean) / (self.c_std)
        else:
            c_norm = (c_aligned - self.c_mean) / (self.c_std + 1e-8)

        # 4) logF0 ì •ê·œí™” & V/UV
        vuv = (f0 > 0).astype(np.float32)
        logf0_norm = np.zeros_like(f0, dtype=np.float32)
        if np.any(vuv > 0):
            logf0_norm[vuv > 0] = (np.log(f0[vuv > 0] + 1e-8) - self.lg_mean) / (self.lg_std)

        # 5) ë„ë©”ì¸ë³„ í†µê³„ & F0 ë³€í™˜
        if cfg.direction.upper() == "A2B":
            mean_log_src, std_log_src = self.logf0_A_mean, self.logf0_A_std
            mean_log_tgt, std_log_tgt = self.logf0_B_mean, self.logf0_B_std
            mcep_mean_tgt, mcep_std_tgt = self.mB_mean, self.mB_std
        elif cfg.direction.upper() == "B2A":
            mean_log_src, std_log_src = self.logf0_B_mean, self.logf0_B_std
            mean_log_tgt, std_log_tgt = self.logf0_A_mean, self.logf0_A_std
            mcep_mean_tgt, mcep_std_tgt = self.mA_mean, self.mA_std
        else:
            raise ValueError("directionì€ 'A2B' ë˜ëŠ” 'B2A' ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        f0_conv = convert_pitch_statistically(f0, mean_log_src, std_log_src, mean_log_tgt, std_log_tgt)

        # 6) ë””ì½”ë” ì¶”ë¡  (íƒ€ê¹ƒ í‰ê·  ìŠ¤íƒ€ì¼ ë²¡í„° ì£¼ì…)
        c_tensor = torch.from_numpy(c_norm).unsqueeze(0).to(self.device)             # (1,Cc,T)
        l_tensor = torch.from_numpy(logf0_norm).reshape(1,1,T).to(self.device)       # (1,1,T)
        v_tensor = torch.from_numpy(vuv).reshape(1,1,T).to(self.device)              # (1,1,T)
        style_vec = self.target_style                                              # (1,E)

        mcep_norm_pred = self.decoder(c_tensor, l_tensor, vuv=v_tensor, style=style_vec)  # (1,36,T)

        # 7) ì—­ì •ê·œí™” & WORLD í•©ì„±
        mcep_pred = mcep_norm_pred.squeeze(0).cpu().numpy() * (mcep_std_tgt + 1e-8) + mcep_mean_tgt  # (36,T)
        coded_sp_conv = np.ascontiguousarray(mcep_pred.T).astype(np.float64)  # (T,36)
        dec_sp = world_decode_sp(coded_sp_conv, cfg.sr)
        y = world_synthesize(f0_conv.astype(np.float64),
                             dec_sp.astype(np.float64),
                             ap.astype(np.float64),
                             cfg.sr, cfg.frame_period)

        # 8) ì•ˆì „ ë ˆë²¨ë§/ë¬´ìŒ ê°€ë“œ
        peak = float(np.max(np.abs(y)) + 1e-9)
        if peak > 1.0:
            y = y / peak
        rms = float(np.sqrt(np.mean(y*y) + 1e-12))
        if rms < 1e-4:
            print(f"[{now()}] âš ï¸ ì¶œë ¥ì´ ë§¤ìš° ì‘ìŠµë‹ˆë‹¤ (rms={rms:.2e}). í†µê³„/í•˜ì´í¼/ì…ë ¥ë ˆë²¨ì„ í™•ì¸í•˜ì„¸ìš”.")
        return y

    def run(self):
        ensure_dir(self.cfg.output_dir)
        wav_files = sorted([f for f in os.listdir(self.cfg.input_dir) if f.lower().endswith('.wav')])
        if not wav_files:
            print(f"âš ï¸ '{self.cfg.input_dir}' í´ë”ì— ë³€í™˜í•  wav íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        print(f"[{now()}] â–¶ï¸ ë³€í™˜ ì‹œì‘: {len(wav_files)}ê°œ | {self.cfg.direction} | "
              f"'{self.cfg.input_dir}' -> '{self.cfg.output_dir}'")
        for i, fname in enumerate(wav_files, 1):
            t0 = time.time()
            try:
                in_path = os.path.join(self.cfg.input_dir, fname)
                y = self.convert_file(in_path)
                out_name, _ = os.path.splitext(fname)
                out_path = os.path.join(self.cfg.output_dir, f"{out_name}_converted_{self.cfg.direction}.wav")
                sf.write(out_path, y, self.cfg.sr)
                print(f"[{now()}] ({i}/{len(wav_files)}) âœ… {fname} -> {os.path.basename(out_path)} "
                      f"({time.time()-t0:.2f}s)")
\
\






# ===== MAIN =====
if __name__ == "__main__":
    # âš™ï¸ MAIN CONFIGURATION
    config = TestConfig(
        input_dir="my_version_song",
        output_dir="converted_outputs/my_song_converted_to_50s",
        direction="A2B",  # "A2B" or "B2A"
        decoder_ckpt="./checkpoints_recon/content_decoder_final.pth",
        cache_dir="./cache",
        # í•˜ì´í¼ëŠ” train.pyì™€ ë°˜ë“œì‹œ ì¼ì¹˜
        sr=16000, frame_period=5.0, mcep_dim=36,
        content_dim=768, style_dim=128, channels=256, n_resblocks=8, use_vuv=True,
        # í‰ê·  ìŠ¤íƒ€ì¼ ì„¤ì • (í•„ìš”ì‹œ ì¡°ì ˆ)
        n_style_refs=16, style_seg_len=192,
    )
    converter = Converter(config)
    converter.run()



'''
# test.py â€” WORLD í•©ì„± ì œê±° + HiFi-GANìœ¼ë¡œ ìµœì¢… íŒŒí˜• í•©ì„± (ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ ìœ ì§€)
# -*- coding: utf-8 -*-
import os
import time
from dataclasses import dataclass
from typing import Optional

import numpy as np
import soundfile as sf
import librosa
import torch
import pyworld
import torch.nn as nn

# (NEW) HiFi-GANìš© ë™ì  ì„í¬íŠ¸ì™€ ë©œ ë³€í™˜ ìœ í‹¸
import json
import librosa.filters as lfilters
import importlib.util
from pathlib import Path

# =========================
# dict -> attribute ì ‘ê·¼ ìœ í‹¸ (HiFi-GAN modelsê°€ ì  í‘œê¸° ì ‘ê·¼ì„ ì‚¬ìš©)
# =========================
class AttrDict(dict):
    """d['x']ì™€ d.x ë‘˜ ë‹¤ ë˜ëŠ” ë˜í¼. ì¤‘ì²© dictë„ ì¬ê·€ ë³€í™˜."""
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        for k, v in list(self.items()):
            if isinstance(v, dict):
                self[k] = AttrDict(v)
            elif isinstance(v, list):
                self[k] = [AttrDict(x) if isinstance(x, dict) else x for x in v]
    def __getattr__(self, item):
        try:
            return self[item]
        except KeyError as e:
            raise AttributeError(item) from e
    __setattr__ = dict.__setitem__
    __delattr__ = dict.__delitem__

# =========================
# HiFi-GAN models.py ë™ì  ë¡œë“œ
# =========================
def load_hifigan_models(repo_root: str):
    """
    repo_root ì•ˆì˜ utils.pyë¥¼ ë¨¼ì € ë¡œë“œí•˜ì—¬ sys.modules['utils']ì— ì£¼ì…í•œ ë’¤,
    models.pyë¥¼ íŒŒì¼ ê²½ë¡œë¡œ ì§ì ‘ ë¡œë“œí•œë‹¤.
    ë°˜í™˜: (Generator, remove_weight_norm or None)
    """
    import sys, importlib.util
    from pathlib import Path

    repo = Path(repo_root).resolve()
    if not repo.exists():
        raise FileNotFoundError(f"hifigan repo not found: {repo_root}")

    # 1) utils.py ì°¾ê¸° (ë£¨íŠ¸ ìš°ì„ , ì—†ìœ¼ë©´ ì¬ê·€ íƒìƒ‰)
    utils_py = repo / "utils.py"
    if not utils_py.is_file():
        for p in repo.rglob("utils.py"):
            utils_py = p
            break
    if not utils_py.is_file():
        raise ModuleNotFoundError(f"HiFi-GAN utils.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {repo_root}")

    # 2) utils.py ë¡œë“œ â†’ sys.modules['utils']ì— ì£¼ì… (models.pyê°€ from utils import ... í•  ë•Œ ì“°ì´ë„ë¡)
    spec_u = importlib.util.spec_from_file_location("hifigan_utils", str(utils_py))
    mod_u = importlib.util.module_from_spec(spec_u)
    assert spec_u and spec_u.loader, "importlib spec/loader ìƒì„± ì‹¤íŒ¨(utils)"
    spec_u.loader.exec_module(mod_u)
    # ì™¸ë¶€ íŒ¨í‚¤ì§€ utilsë³´ë‹¤ ìš°ë¦¬ê°€ ë¡œë“œí•œ ê±¸ ìš°ì„  ì‚¬ìš©í•˜ê²Œ ë®ì–´ì“°ê¸°
    sys.modules["utils"] = mod_u

    # 3) models.py ì°¾ê¸° (ë£¨íŠ¸ ìš°ì„ , ì—†ìœ¼ë©´ ì¬ê·€ íƒìƒ‰)
    models_py = repo / "models.py"
    if not models_py.is_file():
        for p in repo.rglob("models.py"):
            models_py = p
            break
    if not models_py.is_file():
        raise ModuleNotFoundError(f"HiFi-GAN models.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {repo_root}")

    # 4) models.py ë¡œë“œ
    spec_m = importlib.util.spec_from_file_location("hifigan_models", str(models_py))
    mod_m = importlib.util.module_from_spec(spec_m)
    assert spec_m and spec_m.loader, "importlib spec/loader ìƒì„± ì‹¤íŒ¨(models)"
    spec_m.loader.exec_module(mod_m)

    Generator = getattr(mod_m, "Generator")
    remove_weight_norm = getattr(mod_m, "remove_weight_norm", None)
    return Generator, remove_weight_norm


# =========================
# Content ì„ë² ë”© (ê¸°ì¡´ ìœ ì§€: torchaudio Wav2Vec2)
# =========================
try:
    import torchaudio
    from torchaudio.functional import resample as ta_resample
    from torchaudio.pipelines import WAV2VEC2_BASE
    _TORCHAUDIO_OK = True
except ImportError:
    _TORCHAUDIO_OK = False

from data_preprocess import CacheIO  # ìºì‹œ ë¡œë”
from model import ContentDecoder, DecoderCfg  # ëª¨ë¸ êµ¬ì¡°

# =========================
# Utils & WORLD Codec (ë¶„í•´/ë³µì›ë§Œ ì‚¬ìš©)
# =========================
def now() -> str:
    return time.strftime("%H:%M:%S")

def ensure_dir(path: str):
    os.makedirs(path, exist_ok=True)

def pad_wav_to_multiple(wav: np.ndarray, sr: int, frame_period_ms: float, multiple: int = 4) -> np.ndarray:
    frame_size = int(sr * frame_period_ms / 1000)
    n = len(wav)
    padded_frames = int((np.ceil((np.floor(n / frame_size) + 1) / multiple + 1) * multiple - 1) * frame_size)
    pad = max(padded_frames - n, 0)
    if pad <= 0: return wav
    L = pad // 2
    R = pad - L
    return np.pad(wav, (L, R), mode="constant", constant_values=0.0)

def world_decompose(wav: np.ndarray, sr: int, frame_period_ms: float = 5.0):
    wav64 = wav.astype(np.float64)
    f0, timeaxis = pyworld.harvest(wav64, sr, frame_period_ms)
    sp = pyworld.cheaptrick(wav64, f0, timeaxis, sr)
    ap = pyworld.d4c(wav64, f0, timeaxis, sr)
    return f0, sp, ap

def world_decode_sp(coded_sp: np.ndarray, sr: int, mcep_dim: int):
    # coded_sp expected (T, dim) float64, but (dim, T)ë„ ì‹¤ì œë¡œ í†µê³¼ë˜ë˜ ì ì„ ìœ ì§€
    fftlen = pyworld.get_cheaptrick_fft_size(sr)
    decoded_sp = pyworld.decode_spectral_envelope(coded_sp, sr, fftlen)
    return decoded_sp

def convert_pitch_statistically(f0: np.ndarray,
                                mean_log_src: float, std_log_src: float,
                                mean_log_tgt: float, std_log_tgt: float) -> np.ndarray:
    out = np.copy(f0)
    idx = f0 > 0
    if np.any(idx):
        out[idx] = np.exp((np.log(f0[idx] + 1e-8) - mean_log_src) / (std_log_src + 1e-8) * std_log_tgt + mean_log_tgt)
    return out

# =========================
# Content Embedder (ê¸°ì¡´ ìœ ì§€)
# =========================
class ContentEmbedder:
    def __init__(self, device: torch.device):
        if not _TORCHAUDIO_OK:
            raise RuntimeError("ContentEmbedder requires torchaudio. Please install it.")
        bundle = WAV2VEC2_BASE
        self.model = bundle.get_model().to(device).eval()
        self.target_sr = bundle.sample_rate
        self.device = device

    @torch.inference_mode()
    def extract(self, wav: np.ndarray, sr: int) -> np.ndarray:
        t = torch.from_numpy(wav).float().unsqueeze(0).to(self.device)
        if sr != self.target_sr:
            t = ta_resample(t, sr, self.target_sr)
        embs = self.model(t)[0].squeeze(0).cpu().numpy()
        return embs  # (T', C)

    @staticmethod
    def align_to_length(feat_TxC: np.ndarray, L: int) -> np.ndarray:
        T, C = feat_TxC.shape
        if T == L:
            return feat_TxC.T.astype(np.float32)  # (C, T)
        xp = np.linspace(0.0, 1.0, T)
        xq = np.linspace(0.0, 1.0, L)
        out = np.empty((C, L), dtype=np.float32)
        for ch in range(C):
            out[ch] = np.interp(xq, xp, feat_TxC[:, ch])
        return out  # (C, L)

# =========================
# HiFi-GAN Vocoder ë˜í¼
# =========================
class HiFiGANVocoder:
    """
    jik876/HiFi-GAN êµ¬ì¡° í˜¸í™˜. config.jsonì˜ ì˜¤ë””ì˜¤ íŒŒë¼ë¯¸í„°ì— ë§ì¶° ë©œ ë³€í™˜ í›„ Generatorë¡œ í•©ì„±.
    """
    def __init__(self, repo_root: str, ckpt_path: str, config_path: str, device: torch.device):
        # 1) ëª¨ë¸ ì†ŒìŠ¤ ë¡œë“œ
        Generator, remove_weight_norm = load_hifigan_models(repo_root)

        # 2) config ë¡œë“œ
        with open(config_path, "r", encoding="utf-8") as f:
            cfg_dict = json.load(f)

        # 3) ì˜¤ë””ì˜¤ íŒŒë¼ë¯¸í„° (ë©œ íˆ¬ì˜/í”„ë ˆì„ ì •ë ¬ìš©)
        audio = cfg_dict.get("audio", cfg_dict)
        self.sr   = int(audio.get("sampling_rate", 22050))
        self.nfft = int(audio.get("filter_length", 1024))
        self.hop  = int(audio.get("hop_length", 256))
        self.win  = int(audio.get("win_length", 1024))
        self.nmel = int(audio.get("num_mels", 80))
        self.fmin = float(audio.get("mel_fmin", 0.0))
        self.fmax = float(audio.get("mel_fmax", self.sr // 2))

        # 4) Generator ìƒì„± (AttrDictë¡œ í•˜ì´í¼ ì „ë‹¬)
        h = AttrDict(cfg_dict)
        self.device = device if torch.cuda.is_available() else torch.device("cpu")
        self.gen = Generator(h).to(self.device)

        # 5) ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ë‹¤ì–‘í•œ í‚¤ í˜•íƒœ ëŒ€ì‘)
        states = torch.load(ckpt_path, map_location=self.device)
        if isinstance(states, dict):
            sd = states.get("generator", states.get("state_dict", states))
        else:
            sd = states
        self.gen.load_state_dict(sd, strict=False)
        self.gen.eval()
        try:
            if remove_weight_norm:
                remove_weight_norm(self.gen)
        except Exception:
            pass

        # 6) ì„ í˜•â†’ë©œ ë³€í™˜ìš© ê³ ì • í•„í„° (WORLD ë³µì› ìŠ¤í™ì„ ë©œë¡œ íˆ¬ì˜)
        self.mel_basis = lfilters.mel(
            sr=self.sr, n_fft=self.nfft, n_mels=self.nmel,
            fmin=self.fmin, fmax=self.fmax, htk=True, norm=None
        ).astype(np.float32)

    @staticmethod
    def _log10(x: np.ndarray, eps: float = 1e-9) -> np.ndarray:
        return np.log10(np.maximum(x, eps))

    @torch.no_grad()
    def infer_from_linear_spectrogram(self, sp_lin_TxF: np.ndarray, dur_sec: float) -> np.ndarray:
        """
        sp_lin_TxF: [T_mcep, nfft//2+1] â€” WORLD decodeë¡œ ì–»ì€ ì„ í˜• ìŠ¤í™íŠ¸ëŸ¼ (ì‹œê°„ì¶•=ì²«ë²ˆì§¸ ì°¨ì›)
        dur_sec: ì…ë ¥ wav ê¸¸ì´(ì´ˆ). mel í”„ë ˆì„ ìˆ˜ ê³„ì‚°ì— ì‚¬ìš©.
        """
        # [freq, T_mcep]ë¡œ ì „ì¹˜ â†’ mel_basis @ ... â†’ [n_mels, T_mcep]
        mel = (self.mel_basis @ sp_lin_TxF.T).astype(np.float32)
        mel = np.maximum(mel, 1e-9)
        mel = self._log10(mel)

        # HiFi-GAN ë©œ í”„ë ˆì„ ìˆ˜ë¡œ ì‹œê°„ì¶• ë³´ê°„
        T_mel = int(np.round(dur_sec * self.sr / self.hop))
        if mel.shape[1] != T_mel:
            src = np.linspace(0.0, 1.0, mel.shape[1], endpoint=True)
            tgt = np.linspace(0.0, 1.0, T_mel, endpoint=True)
            mel = np.stack([np.interp(tgt, src, mel[ch]) for ch in range(mel.shape[0])], axis=0).astype(np.float32)

        m = torch.from_numpy(mel[None, ...]).to(self.device)  # [1, n_mels, T]
        y = self.gen(m).squeeze().detach().cpu().numpy().astype(np.float32)
        return y

# =========================
# Test Configuration (ê¸°ì¡´ + HiFi-GAN ê²½ë¡œ ì¶”ê°€)
# =========================
@dataclass
class TestConfig:
    # --- ê²½ë¡œ ì„¤ì • ---
    cache_dir: str = "cache"
    decoder_ckpt: str = "checkpoints_recon/content_decoder_final.pth"
    style_embedder_ckpt: str = "checkpoints_recon/style_embedder_final.pth"

    # (NEW) HiFi-GAN ê²½ë¡œ
    hifigan_repo: str = "third_party/hifigan"
    hifigan_ckpt: str = "third_party\hifigan\generator_universal_v1"
    hifigan_config: str = "third_party/hifigan/config.json"
    
    # --- ë³€í™˜ ì„¤ì • ---
    input_dir: str = "my_version_song"              # ë³€í™˜í•  wav íŒŒì¼ì´ ìˆëŠ” í´ë”
    output_dir: str = "converted_outputs/my_song_50s"
    direction: str = "A2B"                          # "A2B": 20ëŒ€->50ëŒ€, "B2A": 50ëŒ€->20ëŒ€

    # --- ëª¨ë¸ ë° ì˜¤ë””ì˜¤ íŒŒë¼ë¯¸í„° ---
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    sr: int = 16000
    frame_period: float = 5.0
    mcep_dim: int = 36
    # ì•„ë˜ ê°’ë“¤ì€ train.pyì˜ ReconConfigì™€ ì¼ì¹˜í•´ì•¼ í•¨
    content_dim: int = 768  # Wav2Vec2-base
    style_dim: int = 128
    channels: int = 256
    n_resblocks: int = 8
    use_vuv: bool = True

# =========================
# Conversion Engine
# =========================
class Converter:
    def __init__(self, cfg: TestConfig):
        self.cfg = cfg
        self.device = torch.device(cfg.device)

        print(f"[{now()}] ğŸ”§ ì´ˆê¸°í™” ì‹œì‘...")
        # 1) ëª¨ë¸ ë¡œë“œ
        self.decoder = self._load_decoder()
        self.style_embedder = self._load_style_embedder()

        # 2) ì½˜í…ì¸  ì„ë² ë”
        self.content_embedder = ContentEmbedder(self.device)

        # 3) ì •ê·œí™” í†µê³„
        self._load_statistics()

        # 4) HiFi-GAN vocoder
        self.vocoder = HiFiGANVocoder(
            repo_root=cfg.hifigan_repo,
            ckpt_path=cfg.hifigan_ckpt,
            config_path=cfg.hifigan_config,
            device=self.device
        )
        print(f"[{now()}] âœ… ì´ˆê¸°í™” ì™„ë£Œ.")

    def _load_decoder(self) -> ContentDecoder:
        dcfg = DecoderCfg(
            content_dim=self.cfg.content_dim, pitch_dim=1,
            vuv_dim=1 if self.cfg.use_vuv else 0, style_dim=self.cfg.style_dim,
            out_dim=self.cfg.mcep_dim, channels=self.cfg.channels,
            n_resblocks=self.cfg.n_resblocks, use_film=True
        )
        decoder = ContentDecoder(dcfg).to(self.device).eval()
        decoder.load_state_dict(torch.load(self.cfg.decoder_ckpt, map_location=self.device))
        return decoder

    def _load_style_embedder(self) -> nn.Embedding:
        embedder = nn.Embedding(2, self.cfg.style_dim).to(self.device).eval()
        embedder.load_state_dict(torch.load(self.cfg.style_embedder_ckpt, map_location=self.device))
        return embedder

    def _load_statistics(self):
        cache = CacheIO.load_preprocessed_data(self.cfg.cache_dir)
        # Content & Global logF0 norm
        self.c_mean = cache["content_normalization"]["mean"].astype(np.float32)
        self.c_std = cache["content_normalization"]["std"].astype(np.float32)
        self.lg_mean = float(cache["logf0_global_normalization"]["mean"])
        self.lg_std = float(cache["logf0_global_normalization"]["std"])
        # MCEP norm per domain
        self.mA_mean, self.mA_std = cache["mcep_norm"]["mean_A"], cache["mcep_norm"]["std_A"]
        self.mB_mean, self.mB_std = cache["mcep_norm"]["mean_B"], cache["mcep_norm"]["std_B"]
        # logF0 stats per domain
        self.logf0_A_mean, self.logf0_A_std = cache["logf0_norm"]["mean_A"], cache["logf0_norm"]["std_A"]
        self.logf0_B_mean, self.logf0_B_std = cache["logf0_norm"]["mean_B"], cache["logf0_norm"]["std_B"]

    @torch.no_grad()
    def convert_file(self, wav_path: str) -> np.ndarray:
        cfg = self.cfg
        
        # 1) ì˜¤ë””ì˜¤ ë¡œë“œ + WORLD ë¶„í•´
        wav, _ = librosa.load(wav_path, sr=cfg.sr, mono=True)
        dur_sec = len(wav) / cfg.sr
        wav_padded = pad_wav_to_multiple(wav, sr=cfg.sr, frame_period_ms=cfg.frame_period)
        f0, sp, ap = world_decompose(wav_padded, sr=cfg.sr, frame_period_ms=cfg.frame_period)
        T = f0.shape[0]  # WORLD í”„ë ˆì„ ìˆ˜

        # 2) íŠ¹ì§• ì¶”ì¶œ/ì •ê·œí™”
        # Content
        c_emb = self.content_embedder.extract(wav, sr=cfg.sr)      # (T', Cc)
        c_aligned = self.content_embedder.align_to_length(c_emb, T)  # (Cc, T)
        c_norm = (c_aligned - self.c_mean) / self.c_std
        
        # logF0 & V/UV
        vuv = (f0 > 0).astype(np.float32)
        logf0_norm = np.zeros_like(f0, dtype=np.float32)
        if np.any(vuv > 0):
            logf0_norm[vuv > 0] = (np.log(f0[vuv > 0] + 1e-8) - self.lg_mean) / self.lg_std

        # 3) ëª©í‘œ ìŠ¤íƒ€ì¼/ìš´ìœ¨(F0)
        if cfg.direction.upper() == "A2B":
            target_domain_id = 1  # B (50ëŒ€)
            mean_log_src, std_log_src = self.logf0_A_mean, self.logf0_A_std
            mean_log_tgt, std_log_tgt = self.logf0_B_mean, self.logf0_B_std
            mcep_mean_tgt, mcep_std_tgt = self.mB_mean, self.mB_std
        elif cfg.direction.upper() == "B2A":
            target_domain_id = 0  # A (20ëŒ€)
            mean_log_src, std_log_src = self.logf0_B_mean, self.logf0_B_std
            mean_log_tgt, std_log_tgt = self.logf0_A_mean, self.logf0_A_std
            mcep_mean_tgt, mcep_std_tgt = self.mA_mean, self.mA_std
        else:
            raise ValueError("directionì€ 'A2B' ë˜ëŠ” 'B2A' ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        
        # F0 ë³€í™˜
        f0_converted = convert_pitch_statistically(f0, mean_log_src, std_log_src, mean_log_tgt, std_log_tgt)

        # 4) ë””ì½”ë” ì¶”ë¡ 
        c_tensor = torch.from_numpy(c_norm).unsqueeze(0).to(self.device)                 # [1, Cc, T]
        l_tensor = torch.from_numpy(logf0_norm).unsqueeze(0).unsqueeze(0).to(self.device)  # [1,1,T]
        v_tensor = torch.from_numpy(vuv).unsqueeze(0).unsqueeze(0).to(self.device)       # [1,1,T]
        domain_tensor = torch.LongTensor([target_domain_id]).to(self.device)
        
        style_vector = self.style_embedder(domain_tensor)
        mcep_norm_pred = self.decoder(c_tensor, l_tensor, vuv=v_tensor, style=style_vector)
        
        # 5) ì—­ì •ê·œí™”
        mcep_pred = mcep_norm_pred.squeeze(0).cpu().numpy() * mcep_std_tgt + mcep_mean_tgt
        coded_sp_converted = np.ascontiguousarray(mcep_pred.T).astype(np.float64)  # (dim, T)

        # 6) WORLD 'ë³µì›'ë§Œ ìˆ˜í–‰: MCEP â†’ ì„ í˜• ìŠ¤í™íŠ¸ëŸ¼ (í•©ì„±ì€ HiFi-GAN)
        decoded_sp = world_decode_sp(coded_sp_converted, cfg.sr, cfg.mcep_dim)  # (T, nfft//2+1)

        # 7) HiFi-GAN: ì„ í˜• ìŠ¤í™íŠ¸ëŸ¼ì„ ë©œë¡œ ë³€í™˜ í›„ ì‹ ê²½ ë³´ì½”ë”©
        y_hifi = self.vocoder.infer_from_linear_spectrogram(decoded_sp, dur_sec=dur_sec)

        # 8) ìµœì¢… SR ë§ì¶”ê¸° (HiFi-GAN í›ˆë ¨ SR â‰  í”„ë¡œì íŠ¸ SRì¼ ìˆ˜ ìˆìŒ)
        if self.vocoder.sr != cfg.sr:
            y_out = librosa.resample(y_hifi, orig_sr=self.vocoder.sr, target_sr=cfg.sr)
        else:
            y_out = y_hifi

        return y_out

    def run(self):
        ensure_dir(self.cfg.output_dir)
        
        wav_files = sorted([f for f in os.listdir(self.cfg.input_dir) if f.lower().endswith('.wav')])
        if not wav_files:
            print(f"âš ï¸ ê²½ê³ : '{self.cfg.input_dir}' í´ë”ì— ë³€í™˜í•  wav íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        print(f"[{now()}] â–¶ï¸ ë³€í™˜ ì‹œì‘: {len(wav_files)}ê°œ íŒŒì¼ | {self.cfg.direction} | "
              f"'{self.cfg.input_dir}' -> '{self.cfg.output_dir}'")
        
        for i, fname in enumerate(wav_files, 1):
            t0 = time.time()
            try:
                in_path = os.path.join(self.cfg.input_dir, fname)
                wav_out = self.convert_file(in_path)
                
                out_fname, _ = os.path.splitext(fname)
                out_path = os.path.join(self.cfg.output_dir, f"{out_fname}_converted_{self.cfg.direction}.wav")
                sf.write(out_path, wav_out, self.cfg.sr)
                
                elapsed = time.time() - t0
                print(f"[{now()}] ({i}/{len(wav_files)}) âœ… ì„±ê³µ: {fname} -> {os.path.basename(out_path)} ({elapsed:.2f}ì´ˆ)")
            except Exception as e:
                print(f"[{now()}] ({i}/{len(wav_files)}) âŒ ì‹¤íŒ¨: {fname}. ì˜¤ë¥˜: {e}")

# =========================
# MAIN
# =========================
if __name__ == "__main__":
    # í•„ìš” ì‹œ ì—¬ê¸°ì„œ ê²½ë¡œ/íŒŒë¼ë¯¸í„°ë§Œ ë°”ê¿”ì„œ ì‹¤í–‰
    config = TestConfig(
        input_dir = "my_version_song",
        output_dir = "converted_outputs/my_song_converted_to_50s",
        direction = "A2B",
        # ê°€ì¤‘ì¹˜/ìºì‹œ ê²½ë¡œ
        decoder_ckpt = "checkpoints_recon/content_decoder_final.pth",
        style_embedder_ckpt = "checkpoints_recon/style_embedder_final.pth",
        cache_dir = "cache",
        # HiFi-GAN ê²½ë¡œ (ë‹¤ìš´ë¡œë“œ/ë°°ì¹˜í•œ íŒŒì¼ëª…ê³¼ ë§ì¶”ê¸°)
        hifigan_repo = "third_party/hifigan",
        hifigan_ckpt = "third_party\hifigan\generator_universal_v1",
        hifigan_config = "third_party/hifigan/config.json",
        # ì˜¤ë””ì˜¤/ëª¨ë¸ íŒŒë¼ë¯¸í„°
        sr = 16000,
        frame_period = 5.0,
        mcep_dim = 36,
        content_dim = 768,
        style_dim = 128,
        channels = 256,
        n_resblocks = 8,
        use_vuv = True,
    )

    converter = Converter(config)
    converter.run()
'''



